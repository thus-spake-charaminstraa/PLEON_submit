{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql_0naMjuQRM"
   },
   "source": [
    "식물 진단 context dictionary\n",
    "0. 물 부족 => 나는 식물이고 물이 필요하다. I'm a plant and I need water\n",
    "1. 과습 => 나는 식물이고 물을 많이 마셨다. I am a plant and I drank a lot of water.\n",
    "2. 햇빛과다 => 나는 식물이고 해가 너무 뜨거워요. I'm a plant and the sun is too hot.\n",
    "3. 습도 부족 => 나는 식물이고 너무 건조해요. I'm a plant and I'm too dry.\n",
    "4. 높은 기온 => 나는 식물이고 너무 더워요. I'm a plant and it's too hot.\n",
    "5. 낮은 기온 => 나는 식물이고 너무 추워요. I'm a plant and it's too cold.\n",
    "6. 영양 부족 => 나는 식물이고 영양소 섭취가 부족해요.I'm a plant and I lack nutrients.\n",
    "7. 영양 과다 => 나는 식물이고 영양소 섭취가 충분해요.I am a plant and I have enough nutrients.\n",
    "8. 병해충 => 나는 식물이고 병해충이 있어요. I am a plant and I have a pest.\n",
    "\n",
    "피드 종류 context dictionary\n",
    "0. 물을 주었음 => 나는 식물이고 물을 마셨다. I am a plant and I drank water.\n",
    "1. 통풍을 해줌 => 나는 식물이고 바람을 쐬었다. I am a plant and got some air.\n",
    "2. 분갈이 => 나는 식물이고 오늘 다른 화분으로 이사를 했다. I am a plant and moved to another pot today.\n",
    "3. 가지치기 => 나는 식물이고 오늘 머리를 잘랐다. I am a plant and I cut my hair today.\n",
    "4. 분무 => 나는 식물이고 물을 마셨다. I am a plant and I drank water.\n",
    "5. 영앙제 => 나는 식물이고 영양제를 먹었다. I am a plant and I took nutritional supplements.\n",
    "6. 오늘의 모습 => 나는 식물이고 너가 오늘 내 사진을 찍어줬다. I am a plant and you took a picture of me today.\n",
    "7. 잎 => 나는 식물이고 잎이 더 많아졌어요. I am a plant and I have more leaves.\n",
    "8. 꽃 => 나는 식물이고 새 꽃이 생겼어요. I am a plant and I have a new flower.\n",
    "9. 열매 => 나는 식물이고 열매가 생겼어요. I am a plant and I have fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1665998180880,
     "user": {
      "displayName": "차준혁",
      "userId": "06162925735462569720"
     },
     "user_tz": -540
    },
    "id": "5C_8fWAa1GpD"
   },
   "outputs": [],
   "source": [
    "ko_doctor_dict = {}\n",
    "ko_feed_dict = {}\n",
    "eng_doctor_dict = {}\n",
    "eng_feed_dict = {}\n",
    "\n",
    "ko_doctor_dict[0] = '나는 목이 마르다.'\n",
    "ko_doctor_dict[1] = '나는 물을 많이 마셨다.'\n",
    "ko_doctor_dict[2] = '요즘 해가 너무 뜨거워요.'\n",
    "ko_doctor_dict[3] = '공기가 너무 건조해요.'\n",
    "ko_doctor_dict[4] = '날씨가 너무 더워요.'\n",
    "ko_doctor_dict[5] = '날씨가 너무 추워요.'\n",
    "ko_doctor_dict[6] = '나는 영양소 섭취가 부족해요.'\n",
    "ko_doctor_dict[7] = '나는 영양소 섭취가 충분해요.'\n",
    "ko_doctor_dict[8] = '나는 병해충이 있어요.'\n",
    "ko_doctor_dict[9] = ''\n",
    "\n",
    "ko_feed_dict[0] = '나는 물을 마셨다.'\n",
    "ko_feed_dict[1] = '나는 바람을 쐬었다.'\n",
    "ko_feed_dict[2] = '나는 오늘 다른 화분으로 이사를 했다.'\n",
    "ko_feed_dict[3] = '나는 오늘 머리를 잘랐다.'\n",
    "ko_feed_dict[4] = '나는 물을 마셨다.'\n",
    "ko_feed_dict[5] = '나는 영양제를 먹었다.'\n",
    "ko_feed_dict[6] = '너가 오늘 내 사진을 찍어줬다.'\n",
    "ko_feed_dict[7] = '나는 잎이 더 많아졌어요.'\n",
    "ko_feed_dict[8] = '나는 새 꽃이 생겼어요.'\n",
    "ko_feed_dict[9] = '나는 열매가 생겼어요.'\n",
    "ko_feed_dict[10] = ''\n",
    "\n",
    "eng_doctor_dict[0] = 'but I am thirsty '\n",
    "eng_doctor_dict[1] = 'but I drank a lot of water '\n",
    "eng_doctor_dict[2] = 'but The sun is too hot '\n",
    "eng_doctor_dict[3] = 'but I am too dry '\n",
    "eng_doctor_dict[4] = 'but It is too hot '\n",
    "eng_doctor_dict[5] = 'but It is too cold '\n",
    "eng_doctor_dict[6] = 'but I lack nutrients '\n",
    "eng_doctor_dict[7] = 'but I have enough nutrients '\n",
    "eng_doctor_dict[8] = 'but I have a pest '\n",
    "eng_doctor_dict[9] = ''\n",
    "\n",
    "eng_feed_dict[0] = 'You gave me water '\n",
    "eng_feed_dict[1] = 'I gave you air '\n",
    "eng_feed_dict[2] = 'You moved me to a new pot '\n",
    "eng_feed_dict[3] = 'You cut off my branches '\n",
    "eng_feed_dict[4] = 'You sprayed me '\n",
    "eng_feed_dict[5] = 'You gave me nutritional supplements '\n",
    "eng_feed_dict[6] = 'You took a picture of me today '\n",
    "eng_feed_dict[7] = 'I have more leaves '\n",
    "eng_feed_dict[8] = 'I have a new flower '\n",
    "eng_feed_dict[9] = 'I have fruit '\n",
    "eng_feed_dict[10] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18986,
     "status": "ok",
     "timestamp": 1665998200330,
     "user": {
      "displayName": "차준혁",
      "userId": "06162925735462569720"
     },
     "user_tz": -540
    },
    "id": "KG1j65XMiwAt",
    "outputId": "bf40f9c4-a873-4731-e8e0-944bf6e0a775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (4.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hyeokiki/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "# all the imports\n",
    "!pip install transformers\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo4dCaYGkx1T"
   },
   "source": [
    "<h1> 1. Dialogpt Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsLf5j8nkxQY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cb06fe84ca4a1299b118cf85d72466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58da16da7dd640238a9a3e477053c46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfa042f1cbf4bcf8639d3b20d19a66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fa92cf34df4098b50d48b59dd214f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c1e55812d84e4e851243c4fa2b3749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca5fdbc85db46609d349bb1c001ef7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849347cd38a3455aa1b6035ea0ecd6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035e7d79b76a4b809bbf3bab2837adff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f4dc53ec944a93aa24efaa8fa30be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1e278b39c141e9b403fa868d43ad89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/808 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b5c2381c3a4c87bf9d16e3fef608db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6103a13b5c06454c830541edb3d6b694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1762c04f0641069d540f6af2aae737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c36ccf01ef4579ac20d16d171bec56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##dialogpt 모델 load\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "trans_tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-3.3B\")\n",
    "trans_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-3.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor_context_num = int(input(\">> doctor context num :\"))\n",
    "feed_context_num = int(input(\">> feed context num ::\"))\n",
    "\n",
    "doctor_context = eng_doctor_dict[doctor_context_num]\n",
    "feed_context = eng_feed_dict[feed_context_num]\n",
    "\n",
    "\n",
    "\n",
    "for step in range(10):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    context = ''\n",
    "    if step == 0:\n",
    "      context = f'Are you my plant?{tokenizer.eos_token} ' \\\n",
    "        + f'Yes, I am your plant {doctor_context}. Can you help me?{tokenizer.eos_token} '\n",
    "    \n",
    "    ko_input = input(\">> User:\")\n",
    "    text_to_translate = ko_input\n",
    "    model_inputs = trans_tokenizer(text_to_translate, return_tensors=\"pt\")\n",
    "    gen_tokens = trans_model.generate(**model_inputs, forced_bos_token_id=trans_tokenizer.lang_code_to_id[\"eng_Latn\"])\n",
    "    en_input = trans_tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(f'사용자 입력 번역 : {en_input}')\n",
    "    \n",
    "    new_user_input_ids = tokenizer.encode(context + en_input[0] + ' How about you?'+ tokenizer.eos_token, return_tensors='pt') if step == 0 else tokenizer.encode(context + en_input[0] + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    print(f'사용 입력 encode : {new_user_input_ids[0]}')\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    \n",
    "    print(f'chat history + 사용 입력 encode : {bot_input_ids[0]}')\n",
    "    \n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f'history : {tokenizer.decode(chat_history_ids[0])}')\n",
    "    print(f'bot : {bot_response}')\n",
    "    # context = 'I am a plant. I bloomed flowers yesterday. I drank a lot of water yesterday. '\n",
    "\n",
    "    # print()\n",
    "    # change = int(input(\">> context 변경? (0 :NO  1 :YES ) :\"))\n",
    "    \n",
    "    # if change == 1 :\n",
    "    #   doctor_context_num = int(input(\">> doctor context num :\"))\n",
    "    #   feed_context_num = int(input(\">> feed context num ::\"))\n",
    "\n",
    "    #   doctor_context = eng_doctor_dict[doctor_context_num]\n",
    "    #   feed_context = eng_feed_dict[feed_context_num]\n",
    "      \n",
    " \n",
    "    # context = f'I am a plant and {feed_context} {doctor_context}'\n",
    "    \n",
    "    # print(f\"context : {context}\")\n",
    "\n",
    "    # temp = tokenizer.encode(context +history + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    text_to_translate = bot_response\n",
    "    model_inputs = trans_tokenizer(text_to_translate, return_tensors=\"pt\")\n",
    "    gen_tokens = trans_model.generate(**model_inputs, forced_bos_token_id=trans_tokenizer.lang_code_to_id[\"kor_Hang\"])\n",
    "    ko_history = trans_tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    # print(f\"history: {history[:]}\")\n",
    "    print(f\"Plant: {ko_history[0]}\")\n",
    "      \n",
    "    # chat_history_ids = temp\n",
    "    \n",
    "    \n",
    "    # print(f\"Plant: {ko_history} ({context})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "1WzANz_WklWM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자 입력 : I gave you fresh air.\n",
      "사용 입력 encode : tensor([ 8491,   345,   616,  4618,    30, 50256,  3363,    11,   314,   716,\n",
      "          534,  4618,   475,   314,   716,  1165,  5894,   764,  1680,   345,\n",
      "         1037,   502,    30, 50256,   314,  2921,   345,  4713,  1633,    13,\n",
      "         1374,   546,   345,    30, 50256])\n",
      "chat history + 사용 입력 encode : tensor([ 8491,   345,   616,  4618,    30, 50256,  3363,    11,   314,   716,\n",
      "          534,  4618,   475,   314,   716,  1165,  5894,   764,  1680,   345,\n",
      "         1037,   502,    30, 50256,   314,  2921,   345,  4713,  1633,    13,\n",
      "         1374,   546,   345,    30, 50256])\n",
      "history : Are you my plant?<|endoftext|> Yes, I am your plant but I am too dry. Can you help me?<|endoftext|> I gave you fresh air. How about you?<|endoftext|>I am a leaf on the wind.<|endoftext|>\n",
      "Plant: I am a leaf on the wind.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자 입력 : Is it ok?\n",
      "사용 입력 encode : tensor([ 3792,   340, 12876,    30, 50256])\n",
      "chat history + 사용 입력 encode : tensor([ 8491,   345,   616,  4618,    30, 50256,  3363,    11,   314,   716,\n",
      "          534,  4618,   475,   314,   716,  1165,  5894,   764,  1680,   345,\n",
      "         1037,   502,    30, 50256,   314,  2921,   345,  4713,  1633,    13,\n",
      "         1374,   546,   345,    30, 50256,    40,   716,   257, 12835,   319,\n",
      "          262,  2344,   764, 50256,  3792,   340, 12876,    30, 50256])\n",
      "history : Are you my plant?<|endoftext|> Yes, I am your plant but I am too dry. Can you help me?<|endoftext|> I gave you fresh air. How about you?<|endoftext|>I am a leaf on the wind.<|endoftext|>Is it ok?<|endoftext|>I am a leaf on the wind.<|endoftext|>\n",
      "Plant: I am a leaf on the wind.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자 입력 : \n",
      "사용 입력 encode : tensor([50256])\n",
      "chat history + 사용 입력 encode : tensor([ 8491,   345,   616,  4618,    30, 50256,  3363,    11,   314,   716,\n",
      "          534,  4618,   475,   314,   716,  1165,  5894,   764,  1680,   345,\n",
      "         1037,   502,    30, 50256,   314,  2921,   345,  4713,  1633,    13,\n",
      "         1374,   546,   345,    30, 50256,    40,   716,   257, 12835,   319,\n",
      "          262,  2344,   764, 50256,  3792,   340, 12876,    30, 50256,    40,\n",
      "          716,   257, 12835,   319,   262,  2344,   764, 50256, 50256])\n",
      "history : Are you my plant?<|endoftext|> Yes, I am your plant but I am too dry. Can you help me?<|endoftext|> I gave you fresh air. How about you?<|endoftext|>I am a leaf on the wind.<|endoftext|>Is it ok?<|endoftext|>I am a leaf on the wind.<|endoftext|><|endoftext|>I am a leaf on the wind.<|endoftext|>\n",
      "Plant: I am a leaf on the wind.\n"
     ]
    }
   ],
   "source": [
    "doctor_context_num = int(input(\">> doctor context num :\"))\n",
    "feed_context_num = int(input(\">> feed context num ::\"))\n",
    "\n",
    "doctor_context = eng_doctor_dict[doctor_context_num]\n",
    "feed_context = eng_feed_dict[feed_context_num]\n",
    "\n",
    "feed_context\n",
    "\n",
    "for step in range(10):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    context = ''\n",
    "    if step == 0:\n",
    "      context = f'Are you my plant?{tokenizer.eos_token} ' \\\n",
    "        + f'Yes, I am your plant {doctor_context}. Can you help me?{tokenizer.eos_token} '\n",
    "    \n",
    "    ko_input = input(\">> User:\")\n",
    "    # text_to_translate = ko_input\n",
    "    # model_inputs = trans_tokenizer(text_to_translate, return_tensors=\"pt\")\n",
    "    # gen_tokens = trans_model.generate(**model_inputs, forced_bos_token_id=trans_tokenizer.lang_code_to_id[\"eng_Latn\"])\n",
    "    # en_input = trans_tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # print(f'사용자 입력 번역 : {en_input}')\n",
    "    \n",
    "    print(f'사용자 입력 : {ko_input}')doctor_context_num = int(input(\">> doctor context num :\"))\n",
    "feed_context_num = int(input(\">> feed context num ::\"))\n",
    "\n",
    "doctor_context = eng_doctor_dict[doctor_context_num]\n",
    "feed_context = eng_feed_dict[feed_context_num]\n",
    "\n",
    "\n",
    "\n",
    "for step in range(10):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    context = ''\n",
    "    if step == 0:\n",
    "      context = f'Are you my plant?{tokenizer.eos_token} ' \\\n",
    "        + f'Yes, I am your plant {doctor_context}. Can you help me?{tokenizer.eos_token} '\n",
    "    \n",
    "    ko_input = input(\">> User:\")\n",
    "    text_to_translate = ko_input\n",
    "    model_inputs = trans_tokenizer(text_to_translate, return_tensors=\"pt\")\n",
    "    gen_tokens = trans_model.generate(**model_inputs, forced_bos_token_id=trans_tokenizer.lang_code_to_id[\"eng_Latn\"])\n",
    "    en_input = trans_tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(f'사용자 입력 번역 : {en_input}')\n",
    "    \n",
    "    new_user_input_ids = tokenizer.encode(context + en_input[0] + ' How about you?'+ tokenizer.eos_token, return_tensors='pt') if step == 0 else tokenizer.encode(context + en_input[0] + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    print(f'사용 입력 encode : {new_user_input_ids[0]}')\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    \n",
    "    print(f'chat history + 사용 입력 encode : {bot_input_ids[0]}')\n",
    "    \n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f'history : {tokenizer.decode(chat_history_ids[0])}')\n",
    "    print(f'bot : {bot_response}')\n",
    "    # context = 'I am a plant. I bloomed flowers yesterday. I drank a lot of water yesterday. '\n",
    "\n",
    "    # print()\n",
    "    # change = int(input(\">> context 변경? (0 :NO  1 :YES ) :\"))\n",
    "    \n",
    "    # if change == 1 :\n",
    "    #   doctor_context_num = int(input(\">> doctor context num :\"))\n",
    "    #   feed_context_num = int(input(\">> feed context num ::\"))\n",
    "\n",
    "    #   doctor_context = eng_doctor_dict[doctor_context_num]\n",
    "    #   feed_context = eng_feed_dict[feed_context_num]\n",
    "      \n",
    " \n",
    "    # context = f'I am a plant and {feed_context} {doctor_context}'\n",
    "    \n",
    "    # print(f\"context : {context}\")\n",
    "\n",
    "    # temp = tokenizer.encode(context +history + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    text_to_translate = bot_response\n",
    "    model_inputs = trans_tokenizer(text_to_translate, return_tensors=\"pt\")\n",
    "    gen_tokens = trans_model.generate(**model_inputs, forced_bos_token_id=trans_tokenizer.lang_code_to_id[\"kor_Hang\"])\n",
    "    ko_history = trans_tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    # print(f\"history: {history[:]}\")\n",
    "    print(f\"Plant: {ko_history[0]}\")\n",
    "      \n",
    "    # chat_history_ids = temp\n",
    "    \n",
    "    \n",
    "    # print(f\"Plant: {ko_history} ({context})\")\n",
    "    \n",
    "    # new_user_input_ids = tokenizer.encode(context + en_input[0] + ' How about you?'+ tokenizer.eos_token, return_tensors='pt') if step == 0 else tokenizer.encode(context + en_input[0] + tokenizer.eos_token, return_tensors='pt')\n",
    "    new_user_input_ids = tokenizer.encode(context + ko_input + ' How about you?'+ tokenizer.eos_token, return_tensors='pt') \\\n",
    "      if step == 0 else tokenizer.encode(context + ko_input  + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    print(f'사용 입력 encode : {new_user_input_ids[0]}')\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    \n",
    "    \n",
    "    print(f'chat history + 사용 입력 encode : {bot_input_ids[0]}')\n",
    "    \n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f'historfeed_contexty : {tokenizer.decode(chat_history_ids[0])}')\n",
    "    # print(f'bot : {bot_response}')\n",
    "    # context = 'I am a plant. I bloomed flowers yesterday. I drank a lot of water yesterday. '\n",
    "\n",
    "    # print()\n",
    "    # change = int(input(\">> context 변경? (0 :NO  1 :YES ) :\"))\n",
    "    \n",
    "    # if change == 1 :\n",
    "    #   doctor_context_num = int(input(\">> doctor context num :\"))\n",
    "    #   feed_context_num = int(input(\">> feed context num ::\"))\n",
    "\n",
    "    #   doctor_context = eng_doctor_dict[doctor_context_num]\n",
    "    #   feed_context = eng_feed_dict[feed_context_num]\n",
    "      \n",
    " \n",
    "    # context = f'I am a plant and {feed_context} {doctor_context}'\n",
    "    \n",
    "    # print(f\"context : {context}\")\n",
    "\n",
    "    # temp = tokenizer.encode(context +history + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    text_to_translate = bot_response\n",
    "    # model_inputs = trans_tokenizer(text_to_translate, return_tensors=\"pt\")\n",
    "    # gen_tokens = trans_model.generate(**model_inputs, forced_bos_token_id=trans_tokenizer.lang_code_to_id[\"kor_Hang\"])\n",
    "    # ko_history = trans_tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    # print(f\"history: {history[:]}\")\n",
    "    print(f\"Plant: {bot_response}\")\n",
    "      \n",
    "    # chat_histofeed_contextry_ids = temp\n",
    "    \n",
    "    \n",
    "    # print(f\"Plant: {ko_history} ({context})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1665973450623,
     "user": {
      "displayName": "차준혁",
      "userId": "06162925735462569720"
     },
     "user_tz": -540
    },
    "id": "ETJ3CEAEwS0u",
    "outputId": "fe03deeb-5f35-416a-8aaa-d0c152b34645"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   40,   716,   257,  4618,   290,   314, 24070,  1660,    13, 10814,\n",
       "          612, 50256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY6yA03Ck8f-"
   },
   "source": [
    "2<h1> 2. Pleon Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8622,
     "status": "ok",
     "timestamp": 1665937605643,
     "user": {
      "displayName": "차준혁",
      "userId": "06162925735462569720"
     },
     "user_tz": -540
    },
    "id": "FW7TQIDWk_5N",
    "outputId": "a3e014e3-f807-430c-b805-247aed5c8bb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:1116: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "##pleon 모델 load\n",
    "\n",
    "from transformerfeed_contexts import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/content/drive/Othercomputers/MacBookPro/chatbot/ko_dialogpt/checkpoint-300000', padding_side='left')\n",
    "model = AutoModelWithLMHead.from_pretrained('/content/drive/Othercomputers/MacBookPro/chatbot/ko_dialogpt/checkpoint-300000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68952,
     "status": "ok",
     "timestamp": 1665937680165,
     "user": {
      "displayName": "차준혁",
      "userId": "06162925735462569720"
     },
     "user_tz": -540
    },
    "id": "bJHbVwy9lFTt",
    "outputId": "a5fde5df-ad69-4fac-8ad9-c240c21870e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:안녕~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: 나는 요즘 회사 일이 바빠서 운동을 못해\n",
      ">> User:많이 바빠?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: 나는 지금 바빠 키키\n",
      ">> User:요즘 회사일 일이 많이 바쁘다며\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: ##면서 왜 바쁘지?\n",
      ">> User:너말이야 너!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: \n",
      ">> User:대답하시지~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: 키키 아니...\n",
      ">> User:뭔소리얌\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: ##며..? 너가 왜 바빠\n",
      ">> User:난 안바빠\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: ㅋ 너가 바빠서 그래..\n",
      ">> User:ㅋ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: 싶네요 키키 아휴 키키 뭐라도 챙겨먹어\n",
      ">> User:나 지금 닭발 먹을거야\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: ##물떡은 너무 질겨\n",
      ">> User:물떡 맛있어.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant: ##바이러스가 유행이래\n"
     ]
    }
   ],
   "source": [
    "for step in range(10):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    tokenizer.encode\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "feed_context\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=1000,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    context = f('I am a plant and' {feed_context})\n",
    "    \n",
    "    chat_history_ids = tokenizer.encode(context +history + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    print(\"Plant: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfAAJI8slsDV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKffNq2Z7CI0Q4K3WpJl8/",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
